#!/home/vlt-os/env/bin/python
"""This file is part of Vulture OS.

Vulture OS is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Vulture OS is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Vulture OS.  If not, see http://www.gnu.org/licenses/.
"""
__author__ = "Th√©o Bertin"
__credits__ = []
__license__ = "GPLv3"
__version__ = "4.0.0"
__maintainer__ = "Vulture OS"
__email__ = "contact@vultureproject.org"
__doc__ = 'Proofpoint POD API Parser'
__parser__ = 'PROOFPOINT POD'


from datetime import timezone, datetime, timedelta
import json
import logging
import signal
import time
from threading import Event
import websocket

from django.conf import settings
from django.utils import timezone, dateparse
from toolkit.api_parser.api_parser import ApiParser
from toolkit.log.log_utils import get_obj_value_or_default

logging.config.dictConfig(settings.LOG_SETTINGS)
logger = logging.getLogger('api_parser')



class ProofpointPodParseError(Exception):
    pass


class ProofpointPodAPIError(Exception):
    pass


class ProofpointPodParser(ApiParser):

    def __init__(self, data):
        super().__init__(data)

        signal.signal(signal.SIGINT, self._handle_stop)
        signal.signal(signal.SIGTERM, self._handle_stop)
        signal.signal(signal.SIGHUP, self._handle_stop)

        self.evt_stop = Event()
        self.ws = None
        self.last_timestamp = None
        self.buffer = list()
        self.ignore_before = None

        proofpoint_pod_uri = data.get('proofpoint_pod_uri')
        proofpoint_pod_cluster_id = data.get('proofpoint_pod_cluster_id')
        proofpoint_pod_token = data.get('proofpoint_pod_token')

        if "https://" in proofpoint_pod_uri:
            proofpoint_pod_uri.replace("https://", "wss://")
        elif "http://" in proofpoint_pod_uri:
            proofpoint_pod_uri.replace("http://", "ws://")
        elif "ws://" not in proofpoint_pod_uri and "wss://" not in proofpoint_pod_uri:
            proofpoint_pod_uri = "wss://" + proofpoint_pod_uri

        self.extra_headers = {
            "Authorization": f"Bearer {proofpoint_pod_token}"
        }

        self.proofpoint_pod_url = f"{proofpoint_pod_uri}?cid={proofpoint_pod_cluster_id}&type=message"


    def __del__(self):
        if self.ws:
            self.ws.close(timeout=1)


    def _handle_stop(self, signum, frame):
        logger.debug(f"[{__parser__}]:_handle_stop: caught signal {signum}, ordering to stop", extra={'frontend': str(self.frontend)})
        self.evt_stop.set()


    def test(self):
        current_time = time.time()
        sinceTime = datetime.now(timezone.utc) - timedelta(hours=2)
        msg = None

        try:
            while time.time() < current_time + 10 and not msg:
                msg = self._websocket_fetch(sinceTime=sinceTime.isoformat())
                if msg:
                    msg = json.loads(msg)
        except Exception as e:
            return {
                'status': False,
                 'error': f'Error while fetching sample message from API: {e}'
            }

        return {
            'status': True,
            'data': [msg]
        }

    def parse_log(self, log):
        try:

            parsed = json.loads(log)
            parsed_ts = dateparse.parse_datetime(parsed['ts'])

            parsed.update({
                'additional': {
                    'related': {
                        'hash': set(),
                        'hosts': set(),
                        'url': set(),
                        'filename': set(),
                    },
                    'email': {
                        'attachments': []
                    },
                    'msgParts': []
                }
            })

            # pop dictionnary, to keep only required fields in result
            for part in parsed.pop('msgParts', []):
                parsed['additional']['related']['hash'].add(part.get('md5', ''))
                parsed['additional']['related']['hash'].add(part.get('sha256', ''))
                for url_obj in part.get('urls', []):
                    url = url_obj.get('url', '')
                    # remove urls that are simply action-links to send a mail
                    if "mailto:" in url:
                        continue
                    # clean url fields to keep only domain and path
                    if '://' in url:
                        url = url.split('://')[1]
                    url = url.split('?')[0]
                    parsed['additional']['related']['url'].add(url)

                parsed['additional']['email']['attachments'].append({
                    'file': {
                        'extension': part.get('detectedExt', '').lower(),
                        'mime_type': part.get('detectedMime', ''),
                        'name': part.get('detectedName', ''),
                        'size': part.get('detectedSizeBytes', ''),
                        'hash': {
                            'md5': part.get('md5', ''),
                            'sha256': part.get('sha256', ''),
                        }
                    }
                })

                parsed['additional']['related']['filename'].add(part.get('detectedName', ''))

                parsed['additional']['msgParts'].append({
                    'name': part.get('detectedName', ''),
                    'isVirtual': part.get('isVirtual', False),
                    'isTimedOut': part.get('isTimedOut', False),
                    'isProtected': part.get('isProtected', False),
                    'isDeleted': part.get('isDeleted', False),
                    'isCorrupted': part.get('isCorrupted', False),
                    'isArchive': part.get('isArchive', False),
                    'metadata': part.get('metadata', {}),
                })

            if results := get_obj_value_or_default(parsed, 'filter.modules.dmarc.alignment.results'):
                for result in results:
                    # check if 'identity' exists (not required according to documentation)
                    if identity := result.get('identity'):
                        parsed['additional']['related']['hosts'].add(identity)
            if results := get_obj_value_or_default(parsed, 'filter.modules.dkimv'):
                for result in results:
                    # no need to check if 'domain' exists (required according to documentation)
                    # still be on the safe side and get() instead of direct index access
                    parsed['additional']['related']['hosts'].add(result.get('domain', ''))
            if result := get_obj_value_or_default(parsed, 'connection.helo'):
                parsed['additional']['related']['hosts'].add(result)

            parsed['additional']['related']['hash'] = list(parsed['additional']['related']['hash'])
            parsed['additional']['related']['hosts'] = list(parsed['additional']['related']['hosts'])
            parsed['additional']['related']['url'] = list(parsed['additional']['related']['url'])
            parsed['additional']['related']['filename'] = list(parsed['additional']['related']['filename'])

            return parsed_ts, parsed
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.warning(f"[{__parser__}]:parse_log: could not parse a log line -> {e}", extra={'frontend': str(self.frontend)})
            logger.debug(f"[{__parser__}]:parse_log: log is {log}", extra={'frontend': str(self.frontend)})
            return None


    def _websocket_fetch(self, sinceTime=None):
        if not self.ws:
            self._websocket_connect(sinceTime=sinceTime)

        try:
            return self.ws.recv()
        except websocket._exceptions.WebSocketTimeoutException:
            logger.debug(f"[{__parser__}]:_websocket_fetch: timeout waiting for log", extra={'frontend': str(self.frontend)})
            return None
        except websocket._exceptions.WebSocketConnectionClosedException:
            raise
        except websocket._exceptions.WebSocketException as e:
            logger.error(f"[{__parser__}]:_websocket_fetch: error while fetching next log -> {e}", extra={'frontend': str(self.frontend)})
            raise ProofpointPodAPIError(f"Could not get next log: {e}")


    def _websocket_connect(self, sinceTime=None):
        if self.ws:
            self.ws.close(timeout=1)
            self.ws = None
        try:
            if sinceTime:
                proofpoint_pod_url = self.proofpoint_pod_url + f"&sinceTime={sinceTime}"
            else:
                proofpoint_pod_url = self.proofpoint_pod_url
            logger.debug(f"[{__parser__}]:_websocket_connect: trying to connect to {proofpoint_pod_url}, with headers {self.extra_headers}", extra={'frontend': str(self.frontend)})
            # timeout is set for connection
            self.ws = websocket.create_connection(proofpoint_pod_url, timeout=10, header=self.extra_headers)
        except websocket._exceptions.WebSocketConnectionClosedException:
            raise
        except websocket._exceptions.WebSocketException as e:
            logger.error(f"[{__parser__}]:_websocket_connect: WebSocket connection error -> {e}", extra={'frontend': str(self.frontend)})
            raise ProofpointPodAPIError(f"Could not connect to API: {e}")

        # lower (web)socket's timeout to set to non-blocking mode and allow quick interruptions
        self.ws.settimeout(1)


    def _flush_buffer(self):
        logger.debug(f"[{__parser__}]:parse_log: current timestamp cursor: {self.last_timestamp.isoformat()}", extra={'frontend': str(self.frontend)})
        if self.buffer:
            self.write_to_file(self.buffer)
            self.buffer.clear()


    def execute(self):
        start_time = time.monotonic()
        logger.info(f"[{__parser__}]:execute: Starting tasks", extra={'frontend': str(self.frontend)})
        stop_after = 50 # stop parser after 50 seconds (allows to refresh code during upgrades)

        current_time = time.time()
        if not self.frontend.last_api_call:
            self.frontend.last_api_call = datetime.now(timezone.utc)
        self.last_timestamp = self.frontend.last_api_call

        # stop parser after 1 hour when last call was more that an hour ago (need time to get bundle of logs for past hours, and will stop once it's done)
        if self.last_timestamp.timestamp() + 3600 < current_time:
            logger.info(f"[{__parser__}]:execute: last call was more than an hour ago, parser will run for (max) 1 hour to catch up with current time", extra={'frontend': str(self.frontend)})
            stop_after = 3600


        try:
            while time.monotonic() < start_time + stop_after and not self.evt_stop.is_set():
                msg = self._websocket_fetch(sinceTime=self.frontend.last_api_call.isoformat())
                if msg:
                    timestamp, msg = self.parse_log(msg)
                    if msg:
                        self.buffer.append(json.dumps(msg))
                        self.last_timestamp = max(self.last_timestamp, timestamp)
                        if len(self.buffer) >= 128:
                            self._flush_buffer()
                else:
                    # no new log received during timeout, flush current buffer
                    logger.debug(f"[{__parser__}]:execute: no logs received for some time, flushing buffer", extra={'frontend': str(self.frontend)})
                    self._flush_buffer()
        except (ProofpointPodAPIError, ProofpointPodParseError) as e:
            raise
        except websocket._exceptions.WebSocketConnectionClosedException:
            logger.info(f"[{__parser__}]:execute: Connection was closed, finalizing...", extra={'frontend': str(self.frontend)})
            pass
        except Exception as e:
            logger.critical(f"[{__parser__}]:execute: Error while processing logs -> {e}", extra={'frontend': str(self.frontend)})
            raise ProofpointPodAPIError("Unknown error: {e}")

        if self.buffer:
            logger.debug(f"[{__parser__}]:execute: flushing remaining logs", extra={'frontend': str(self.frontend)})
            self._flush_buffer()

        self.frontend.last_api_call = self.last_timestamp
        logger.debug(f"[{__parser__}]:execute: Updated frontend timestamp is {self.frontend.last_api_call}", extra={'frontend': str(self.frontend)})

        logger.info(f"[{__parser__}]:execute: Parsing done.", extra={'frontend': str(self.frontend)})
        self.finish()
